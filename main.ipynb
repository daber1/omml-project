{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83588b3",
   "metadata": {},
   "source": [
    "\n",
    "## MARINA and DIANA - 2 girls for unbiased compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec8eae",
   "metadata": {},
   "source": [
    "statement, idea, theory, essence of proof, results of experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94d0b4",
   "metadata": {},
   "source": [
    "describe the problem statement, its importance, examples of its occurrence, describe the main idea of the authors' approach, what it is based on (what works were the basis of this approach), describe the key features of the proposed methods, intuition why they can work well, what are the improvements with the basic versions of what was in the literature before. If summarized you need to deal with the introduction of the papers, the part about countertributions and related works, and look at the proposed methods themselves. No implementation and parsing of the theory/theorem of convergence is needed yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3887d5b",
   "metadata": {},
   "source": [
    "## MARINA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2dbaf",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34a976",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    <p>Non-convex optimization problems, encountered in ML applications such as Deep Neural Network training and matrix completion, have gained substantial attention due to the their practical significance. To enhance the generalization performance of DL models, practitioners increasingly rely on larger datasets and distributed computing resources. However, distributed methods face a challenge - communication bottleneck, where the cost of transmitting information among distributed workers can be higher than computation costs. To address this challenge, comunication compression techniques have been proposed, but their effectiveness depends on achieving a balance between communication savings and increased communications rounds. The problem of designing effecient distributed optimization methods with compression remains a complex and important challenge.</p>\n",
    "    <p>\n",
    "    This paper aims to contribute a novel solution to improve the efficiency of non-convex distributed learning with compression.\n",
    "    </p>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657d3eae",
   "metadata": {},
   "source": [
    "### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e4b235",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    <p>\n",
    "        The main idea is a new distrubuted method called <b>MARINA</b>.\n",
    "        In this algorithm, workers apply an unbiased compression operator to the <i>gradient differences</i> at each iteration with some probability and sent them to the server that performs aggregation by averaging. This procedure leads to a <i>biased</i> gradient estimator. The paper proves convergence guarantees for MARINA, showing that its performance is strictly better than previous state-of-the-art methods. The convergence rate of MARINA is significantly improved, particularly when compared to methods like DIANA.\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>Variance reduction:</b> The paper extends MARINA to VR-MARINA, which can handle scenarios where local functions take the form of either finite sums or expectations. VR-MARINA performs local variance reduction on all nodes, progressively reducing the variance introduced by stochastic approximation. This results in improved oracle complexity compared to previous state-of-the-art methods, especially when no compression is used or when significant compression is applied.\n",
    "        </p>\n",
    "    <p>\n",
    "        <b>Partial Participation:</b> A modification of MARINA called PP-MARINA is introduced, allowing for partial participation of clients. This feature is critical in federated learning scenarios. PP-MARINA achieves superior communication complexity compared to existing methods designed for similar settings.\n",
    "        </p>\n",
    "    <p>\n",
    "        <b>Convergence Under the Polyak-Łojasiewicz Condition:</b> The paper analyzes all proposed methods for problems satisfying the Polyak-Łojasiewicz condition, and the obtained results are shown to be strictly better than previous ones.\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>Simple Analysis:</b>The paper highlights the simplicity and flexibility of the analysis, making it possible to extend the approach to different quantization operators and batch sizes used by clients. The paper also suggests the possibility of combining the ideas from VR-MARINA and PP-MARINA to create a unified distributed algorithm with compressed communications, variance reduction on nodes, and client sampling.\n",
    "    </p>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af6fc2f",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d764c",
   "metadata": {},
   "source": [
    "### Essence of proof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d2fcf2",
   "metadata": {},
   "source": [
    "### Results of experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8cb4f4",
   "metadata": {},
   "source": [
    "## DIANA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fab56c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
