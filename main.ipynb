{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b51eea3",
   "metadata": {},
   "source": [
    "\n",
    "## MARINA and DIANA - 2 girls for unbiased compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f98bb",
   "metadata": {},
   "source": [
    "statement, idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c5fcb",
   "metadata": {},
   "source": [
    "describe the problem statement, its importance, examples of its occurrence, describe the main idea of the authors' approach, what it is based on (what works were the basis of this approach), describe the key features of the proposed methods, intuition why they can work well, what are the improvements with the basic versions of what was in the literature before. If summarized you need to deal with the introduction of the papers, the part about countertributions and related works, and look at the proposed methods themselves. No implementation and parsing of the theory/theorem of convergence is needed yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4267eb32",
   "metadata": {},
   "source": [
    "## MARINA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2191bc",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa57d8",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    <p>Non-convex optimization problems, encountered in ML applications such as Deep Neural Network training and matrix completion, have gained substantial attention due to the their practical significance. To enhance the generalization performance of DL models, practitioners increasingly rely on larger datasets and distributed computing resources. However, distributed methods face a challenge - communication bottleneck, where the cost of transmitting information among distributed workers can be higher than computation costs. To address this challenge, communication compression techniques have been proposed, but their effectiveness depends on achieving a balance between communication savings and increased communications rounds. The problem of designing effecient distributed optimization methods with compression remains a complex and important challenge.</p>\n",
    "    <p>\n",
    "    This paper aims to contribute a novel solution to improve the efficiency of non-convex distributed learning with compression.\n",
    "    </p>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec279856",
   "metadata": {},
   "source": [
    "### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d44603",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    <p>\n",
    "        The main idea is a new distrubuted method called <b>MARINA</b>.\n",
    "        In this algorithm, workers apply an unbiased compression operator to the <i>gradient differences</i> at each iteration with some probability and sent them to the server that performs aggregation by averaging. This procedure leads to a <i>biased</i> gradient estimator. The paper proves convergence guarantees for MARINA, showing that its performance is strictly better than previous state-of-the-art methods. The convergence rate of MARINA is significantly improved, particularly when compared to methods like DIANA.\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>Variance reduction:</b> The paper extends MARINA to VR-MARINA, which can handle scenarios where local functions take the form of either finite sums or expectations. VR-MARINA performs local variance reduction on all nodes, progressively reducing the variance introduced by stochastic approximation. This results in improved oracle complexity compared to previous state-of-the-art methods, especially when no compression is used or when significant compression is applied.\n",
    "        </p>\n",
    "    <p>\n",
    "        <b>Partial Participation:</b> A modification of MARINA called PP-MARINA is introduced, allowing for partial participation of clients. This feature is critical in federated learning scenarios. PP-MARINA achieves superior communication complexity compared to existing methods designed for similar settings.\n",
    "        </p>\n",
    "    <p>\n",
    "        <b>Convergence Under the Polyak-Łojasiewicz Condition:</b> The paper analyzes all proposed methods for problems satisfying the Polyak-Łojasiewicz condition, and the obtained results are shown to be strictly better than previous ones.\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>Simple Analysis:</b>The paper highlights the simplicity and flexibility of the analysis, making it possible to extend the approach to different quantization operators and batch sizes used by clients. The paper also suggests the possibility of combining the ideas from VR-MARINA and PP-MARINA to create a unified distributed algorithm with compressed communications, variance reduction on nodes, and client sampling.\n",
    "    </p>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0788d",
   "metadata": {},
   "source": [
    "#### Based on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a1d0f2",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    <p>\n",
    "    </p>\n",
    "    <li><b>Non-Convex Optimization:</b> The paper builds upon the field of non-convex optimization, which involves finding optimal solutions for functions that are not necessarily convex.</li>\n",
    "    <li><b>Compressed Communications:</b> The paper extends its work from the domain of distributed optimization to include communication compression techniques. It categorizes compression methods into two groups: unbiased compression operators (quantizations) and biased compressors (e.g., TopK). The paper explores the benefits and drawbacks of these techniques.</li>\n",
    "    <li><b>Unbiased Compression:</b> Within the realm of communication compression, the paper discusses methods that employ unbiased compression operators. It references prior research in this area, including the introduction of DIANA and VR-DIANA, which have been used for non-convex optimization problems.</li>\n",
    "    <li><b>Biased Compression:</b> The paper also delves into biased compression operators, which are considered less optimization-friendly than their unbiased counterparts. It highlights the challenges associated with biased compressors and discusses error compensation techniques to address these issues. The paper references research that removes some assumptions related to the boundedness of stochastic gradients.</li>\n",
    "    <li><b>Other Approaches:</b> The paper recognizes that communication compression is not the only technique for reducing communication costs in distributed optimization. It mentions decentralized communications and multiple local steps between communication rounds as alternative strategies, particularly relevant in federated learning.</li>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e05b4eb",
   "metadata": {},
   "source": [
    "#### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6452b",
   "metadata": {},
   "source": [
    "1. **Communication Compression:** The core intuition behind these methods is the effective use of communication compression. In distributed optimization, a significant bottleneck is often the high communication cost required to exchange information among distributed workers. By compressing the information transmitted during communication, the methods reduce the amount of data that needs to be exchanged. This reduction in communication overhead can lead to significant speedup and efficiency gains.\n",
    "2. **Bias in Gradient Estimation:** The paper explores the idea of introducing bias into gradient estimations during communication compression. While biased estimations might seem counterintuitive in optimization, the methods leverage this bias strategically to achieve faster convergence. By allowing the gradient information to be compressed in a biased manner, they reduce the communication burden while maintaining convergence guarantees.\n",
    "3. **Variance Reduction:** Variance reduction techniques, as seen in VR-MARINA, are employed to further improve convergence rates. These techniques aim to reduce the variance introduced by stochastic approximations. By addressing variance issues inherent in stochastic optimization, the methods can converge faster and more reliably, even when communication is compressed.\n",
    "4. **Flexibility for Heterogeneous Local Loss Functions:** One critical intuition is the ability of the methods to handle scenarios with heterogeneous local loss functions. In real-world distributed settings, it's common for each worker to have a different local loss function. The methods introduced in the paper are designed to adapt to this heterogeneity, allowing for more practical and versatile applications in distributed machine learning.\n",
    "5. **Partial Participation:** The introduction of methods like PP-MARINA, which allow for partial participation of clients, is intuitive in scenarios like federated learning. In federated settings, not all clients may participate in every round of communication. Enabling partial participation reduces unnecessary communication overhead and improves efficiency.\n",
    "6. **Convergence Analysis:** The methods are grounded in rigorous convergence analysis. The paper provides theoretical guarantees that demonstrate the convergence of these methods, even in non-convex optimization scenarios. These guarantees provide confidence that the proposed techniques will lead to solutions with acceptable accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31b8fd",
   "metadata": {},
   "source": [
    "## DIANA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd3f8a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
