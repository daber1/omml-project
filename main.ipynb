{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b51eea3",
   "metadata": {},
   "source": [
    "\n",
    "## MARINA and DIANA - 2 girls for unbiased compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f98bb",
   "metadata": {},
   "source": [
    "statement, idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c5fcb",
   "metadata": {},
   "source": [
    "describe the problem statement, its importance, examples of its occurrence, describe the main idea of the authors' approach, what it is based on (what works were the basis of this approach), describe the key features of the proposed methods, intuition why they can work well, what are the improvements with the basic versions of what was in the literature before. If summarized you need to deal with the introduction of the papers, the part about countertributions and related works, and look at the proposed methods themselves. No implementation and parsing of the theory/theorem of convergence is needed yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4267eb32",
   "metadata": {},
   "source": [
    "## MARINA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2191bc",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa57d8",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    <p>Non-convex optimization problems, encountered in ML applications such as Deep Neural Network training and matrix completion, have gained substantial attention due to the their practical significance. To enhance the generalization performance of DL models, practitioners increasingly rely on larger datasets and distributed computing resources. However, distributed methods face a challenge - communication bottleneck, where the cost of transmitting information among distributed workers can be higher than computation costs. To address this challenge, communication compression techniques have been proposed, but their effectiveness depends on achieving a balance between communication savings and increased communications rounds. The problem of designing effecient distributed optimization methods with compression remains a complex and important challenge.</p>\n",
    "    <p>\n",
    "    This paper aims to contribute a novel solution to improve the efficiency of non-convex distributed learning with compression.\n",
    "    </p>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec279856",
   "metadata": {},
   "source": [
    "### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d44603",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    <p>\n",
    "        The main idea is a new distrubuted method called <b>MARINA</b>.\n",
    "        In this algorithm, workers apply an unbiased compression operator to the <i>gradient differences</i> at each iteration with some probability and sent them to the server that performs aggregation by averaging. This procedure leads to a <i>biased</i> gradient estimator. The paper proves convergence guarantees for MARINA, showing that its performance is strictly better than previous state-of-the-art methods. The convergence rate of MARINA is significantly improved, particularly when compared to methods like DIANA.\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>Variance reduction:</b> The paper extends MARINA to VR-MARINA, which can handle scenarios where local functions take the form of either finite sums or expectations. VR-MARINA performs local variance reduction on all nodes, progressively reducing the variance introduced by stochastic approximation. This results in improved oracle complexity compared to previous state-of-the-art methods, especially when no compression is used or when significant compression is applied.\n",
    "        </p>\n",
    "    <p>\n",
    "        <b>Partial Participation:</b> A modification of MARINA called PP-MARINA is introduced, allowing for partial participation of clients. This feature is critical in federated learning scenarios. PP-MARINA achieves superior communication complexity compared to existing methods designed for similar settings.\n",
    "        </p>\n",
    "    <p>\n",
    "        <b>Convergence Under the Polyak-Łojasiewicz Condition:</b> The paper analyzes all proposed methods for problems satisfying the Polyak-Łojasiewicz condition, and the obtained results are shown to be strictly better than previous ones.\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>Simple Analysis:</b>The paper highlights the simplicity and flexibility of the analysis, making it possible to extend the approach to different quantization operators and batch sizes used by clients. The paper also suggests the possibility of combining the ideas from VR-MARINA and PP-MARINA to create a unified distributed algorithm with compressed communications, variance reduction on nodes, and client sampling.\n",
    "    </p>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0788d",
   "metadata": {},
   "source": [
    "#### Based on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a1d0f2",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    <p>\n",
    "    </p>\n",
    "    <li><b>Non-Convex Optimization:</b> The paper builds upon the field of non-convex optimization, which involves finding optimal solutions for functions that are not necessarily convex.</li>\n",
    "    <li><b>Compressed Communications:</b> The paper extends its work from the domain of distributed optimization to include communication compression techniques. It categorizes compression methods into two groups: unbiased compression operators (quantizations) and biased compressors (e.g., TopK). The paper explores the benefits and drawbacks of these techniques.</li>\n",
    "    <li><b>Unbiased Compression:</b> Within the realm of communication compression, the paper discusses methods that employ unbiased compression operators. It references prior research in this area, including the introduction of DIANA and VR-DIANA, which have been used for non-convex optimization problems.</li>\n",
    "    <li><b>Biased Compression:</b> The paper also delves into biased compression operators, which are considered less optimization-friendly than their unbiased counterparts. It highlights the challenges associated with biased compressors and discusses error compensation techniques to address these issues. The paper references research that removes some assumptions related to the boundedness of stochastic gradients.</li>\n",
    "    <li><b>Other Approaches:</b> The paper recognizes that communication compression is not the only technique for reducing communication costs in distributed optimization. It mentions decentralized communications and multiple local steps between communication rounds as alternative strategies, particularly relevant in federated learning.</li>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e05b4eb",
   "metadata": {},
   "source": [
    "#### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6452b",
   "metadata": {},
   "source": [
    "1. **Communication Compression:** The core intuition behind these methods is the effective use of communication compression. In distributed optimization, a significant bottleneck is often the high communication cost required to exchange information among distributed workers. By compressing the information transmitted during communication, the methods reduce the amount of data that needs to be exchanged. This reduction in communication overhead can lead to significant speedup and efficiency gains.\n",
    "2. **Bias in Gradient Estimation:** The paper explores the idea of introducing bias into gradient estimations during communication compression. While biased estimations might seem counterintuitive in optimization, the methods leverage this bias strategically to achieve faster convergence. By allowing the gradient information to be compressed in a biased manner, they reduce the communication burden while maintaining convergence guarantees.\n",
    "3. **Variance Reduction:** Variance reduction techniques, as seen in VR-MARINA, are employed to further improve convergence rates. These techniques aim to reduce the variance introduced by stochastic approximations. By addressing variance issues inherent in stochastic optimization, the methods can converge faster and more reliably, even when communication is compressed.\n",
    "4. **Flexibility for Heterogeneous Local Loss Functions:** One critical intuition is the ability of the methods to handle scenarios with heterogeneous local loss functions. In real-world distributed settings, it's common for each worker to have a different local loss function. The methods introduced in the paper are designed to adapt to this heterogeneity, allowing for more practical and versatile applications in distributed machine learning.\n",
    "5. **Partial Participation:** The introduction of methods like PP-MARINA, which allow for partial participation of clients, is intuitive in scenarios like federated learning. In federated settings, not all clients may participate in every round of communication. Enabling partial participation reduces unnecessary communication overhead and improves efficiency.\n",
    "6. **Convergence Analysis:** The methods are grounded in rigorous convergence analysis. The paper provides theoretical guarantees that demonstrate the convergence of these methods, even in non-convex optimization scenarios. These guarantees provide confidence that the proposed techniques will lead to solutions with acceptable accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31b8fd",
   "metadata": {},
   "source": [
    "## DIANA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd3f8a",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574ce70",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    <p>The DIANA paper addresses the challenge of distributed optimization, where the objective function is partitioned across different devices. Each device transmits incremental model updates to a central server. The primary issue lies in the communication bottleneck, prompting recent research to propose various compression techniques, such as quantization or sparsification, for the gradients. However, this compression introduces additional variance denoted as ω (ω ≥ 1), which has the potential to impede convergence. The paper focuses on two specific scenarios: strongly convex functions with a condition number κ distributed across n machines, and objective functions structured as finite sums, with each worker handling fewer than m components. The paper aims to address these challenges by presenting innovative variance-reduced schemes. The proposed schemes demonstrate convergence in a number of steps that are logarithmic in the form O((κ + κωn + ω + m) log(1/ε)), achieving accuracy ε > 0. Notably, these methods achieve linear convergence even when dealing with quantized updates. Additionally, the paper includes analysis for weakly convex and non-convex cases, along with experimental verification showcasing the superior efficiency of the novel variance-reduced schemes compared to baseline approaches.</p></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429e4f2",
   "metadata": {},
   "source": [
    "### Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f995336a",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    <p>\n",
    "        <br><br>\n",
    "    <b>Generalizing Compression Methods:</b>\n",
    "    <p>The paper extends and generalizes the DIANA compression algorithm. While DIANA focused on specific compression methods(block quantization), the paper permits the utilization of arbitrary ω-quantization operators. This advancement allows for a wider choice of quantization strategies, enabling the selection of operators that yield optimal performance concerning the system's resources.</p><br>\n",
    "    <b>Variance Reduction for Quantized Gradient Updates:</b>\n",
    "    <p>The base algorithm(without variance reduction) achieves linear convergence. However, one of its limitations is that it can only guarantee linear convergence to a 2σ²/(nµ(µ+L))-neighborhood of the optimal solution. This implies that the average variance of the workers' stochastic gradients determines the size of the neighborhood. The variance reduced methods proposed in this paper achieve linear convergence for arbitrary accuracy ε > 0. This implies that the proposed variance-reduced methods can converge with a much higher level of precision (arbitrary accuracy). </p><br>\n",
    "    <b>Efficient Communication:</b>\n",
    "    <p>To further optimize communication, the quantization operator is chosen strategically. It's essential that the quantization scheme (Q) is designed such that the transmission of the quantized difference requires significantly fewer bits compared to transmitting the full d-dimensional vector. This helps in reducing the communication overhead, which is critical for distributed optimization.</p>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090aa332",
   "metadata": {},
   "source": [
    "#### Based on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58757229",
   "metadata": {},
   "source": [
    "<font size=2>\n",
    "    <p>\n",
    "    </p>\n",
    "    <li><b>stochastic variance-reduced gradient method (SVRG):</b> The paper builds upon ideas from SVRG algorithm and its accelerated variant (Katyusha) </li>\n",
    "    <li><b>Communication-Efficient SGD:</b> quantization-based compression schemes to reduce communication cost in parallel stochastic gradient descent during large-scale deep learning </li>\n",
    "    <li><b>DIANA:</b> improves distributed optimization by compressing gradient differences, achieving convergence in a neighborhood of the optimal solution with reduced communication cost.</li>\n",
    "    <li><b>SAGA:</b> an efficient optimization algorithm with fast convergence, suitable for composite objectives, including non-strongly convex problems.</li>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9ab2c",
   "metadata": {},
   "source": [
    "#### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eda5a2",
   "metadata": {},
   "source": [
    "1. **Generalizing Compression Methods:** the main intuition behind this modification is to provide greater flexibility and versatility in the compression techniques employed during the distributed optimization process. By allowing arbitrary ω-quantization operators, the modified DIANA algorithm enables the selection of compression methods that suit the available system resources and potentially offer gains in training time.\n",
    "2. **Variance Reduction:** Reducing variance through allows for more stable and accurate updates, which in turn can lead to faster convergence and better optimization performance. The gains in convergence rate are a result of this improved stability and accuracy, ultimately leading to a more efficient and effective optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e94b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
